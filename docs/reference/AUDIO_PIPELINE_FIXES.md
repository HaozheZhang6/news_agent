# Audio Pipeline Fixes - Final Round

## Issues Fixed

### 1. Audio Chunks Sending Too Frequently ‚úÖ

**Problem:**
Frontend was sending multiple small audio chunks (3-5KB) continuously instead of accumulating and sending one chunk after 1 second of silence.

**Root Cause:**
The silence timer logic was using `!silenceTimerRef.current` check, but we had removed the timer mechanism, causing the condition to always be true.

**Fix:**
[ContinuousVoiceInterface.tsx:375-385](frontend/src/components/ContinuousVoiceInterface.tsx#L375-L385)
```typescript
// Removed timer, send immediately when threshold reached
if (silenceDuration >= SILENCE_THRESHOLD_MS && audioChunksRef.current.length > 0) {
  console.log(`üì§ Silence threshold reached (${silenceDuration}ms), sending ${audioChunksRef.current.length} chunks immediately`);

  // Send immediately when silence threshold is reached
  sendAudioToBackend();

  // Reset last speech time to prevent multiple sends
  lastSpeechTimeRef.current = Date.now();
}
```

**Result:**
- Audio now accumulates during speech
- Sends once after 1 second of silence
- No more continuous small chunks

### 2. Default Question Hardcoded in Backend ‚úÖ

**Problem:**
Backend was using fallback transcription "What's the stock price of AAPL today?" when transcription failed, making it look like the user asked that question every time.

**Fix:**
[streaming_handler.py:142-145, 177-181](backend/app/core/streaming_handler.py#L142-L145)
```python
# BEFORE - returned default question
if not self._model_loaded or self.sensevoice_model is None:
    return "What's the stock price of AAPL today?"

# AFTER - raise proper error
if not self._model_loaded or self.sensevoice_model is None:
    print(f"‚ö†Ô∏è SenseVoice model not loaded, transcription unavailable")
    raise RuntimeError("Speech recognition model not loaded. Please check model initialization.")
```

**Result:**
- No more fake transcriptions
- Proper error messages when transcription fails
- User sees actual error instead of wrong question

### 3. FFmpeg Conversion Errors ‚úÖ

**Problem:**
FFmpeg was failing to convert WebM audio chunks with error:
```
Error opening input file: Invalid data found when processing input
```

**Root Cause:**
- Individual WebM chunks from MediaRecorder may not be valid standalone files
- FFmpeg needs better parameters for handling partial audio data

**Fix:**
[streaming_handler.py:212-223](backend/app/core/streaming_handler.py#L212-L223)
```python
# Added better FFmpeg options
ffmpeg_cmd = [
    'ffmpeg',
    '-v', 'error',  # Only show errors
    '-i', input_path,
    '-ar', str(sample_rate),
    '-ac', '1',  # Mono
    '-f', 'wav',
    '-acodec', 'pcm_s16le',  # 16-bit PCM
    '-y',  # Overwrite output
    output_path
]
```

**Note:** This may still fail if individual chunks aren't valid WebM files. Consider implementing audio buffering on backend side.

### 4. Anonymous User ID UUID Error ‚úÖ

**Problem:**
Database errors when user_id was "anonymous":
```
invalid input syntax for type uuid: "anonymous"
```

**Root Cause:**
Supabase database requires valid UUIDs for user_id field, but backend was defaulting to string "anonymous".

**Fix:**
[main.py:186-188](backend/app/main.py#L186-L188)
```python
# BEFORE
user_id = websocket.query_params.get("user_id", "anonymous")

# AFTER
user_id = websocket.query_params.get("user_id", "00000000-0000-0000-0000-000000000000")
```

**Result:**
- Valid UUID format for anonymous users
- No more database errors
- Conversations saved correctly

## Remaining Issues to Address

### 1. WebM Audio Chunk Validity

**Issue:**
Individual WebM chunks may not be valid standalone audio files, causing FFmpeg conversion to fail.

**Potential Solutions:**
1. **Backend Buffering:** Accumulate multiple chunks before transcription
2. **Frontend Format Change:** Use WAV format instead of WebM (larger but more reliable)
3. **Stream Processing:** Use FFmpeg streaming mode to handle partial data

**Recommendation:** Test with WAV format first to verify the pipeline works, then optimize back to WebM if needed.

### 2. SenseVoice Model Loading

**Issue:**
Transcription may fail if SenseVoice model isn't loaded, now raises errors instead of fallback.

**To Verify:**
- Check model initialization in `StreamingHandler.__init__()`
- Ensure model files are available
- Test with actual audio input

## Testing Steps

### 1. Test Audio Chunking

1. Open frontend in browser
2. Click microphone button
3. Speak for 2-3 seconds: "What's the weather today?"
4. Stop speaking and wait 1 second
5. **Expected:** One audio chunk sent after 1 second of silence
6. **Check console:** Should see single "üì§ Silence threshold reached" message

### 2. Test Transcription

1. Send audio as above
2. **Expected:** Your actual speech transcribed, not "AAPL stock price"
3. **If fails:** Check backend logs for transcription errors
4. **If model not loaded:** Error message instead of fake transcription

### 3. Test Database

1. Send audio without user_id parameter
2. **Expected:** Session saved with user_id = "00000000-0000-0000-0000-000000000000"
3. **No errors:** UUID syntax errors should be gone

## Files Modified

1. **[frontend/src/components/ContinuousVoiceInterface.tsx](frontend/src/components/ContinuousVoiceInterface.tsx)**
   - Line 375-385: Fixed audio chunking logic
   - Removed timer mechanism, send immediately when threshold reached
   - Reset lastSpeechTimeRef after sending to prevent multiple sends

2. **[backend/app/core/streaming_handler.py](backend/app/core/streaming_handler.py)**
   - Line 142-145: Remove fallback default question, raise error instead
   - Line 177-181: Remove fallback from empty result and exception handling
   - Line 212-223: Improved FFmpeg parameters for WebM conversion

3. **[backend/app/main.py](backend/app/main.py)**
   - Line 186-188: Use valid UUID for anonymous users

## Summary

All major issues have been addressed:

‚úÖ **Audio chunking** - No more continuous small sends, proper 1-second silence detection
‚úÖ **Default questions** - Removed all hardcoded fallback questions
‚úÖ **User ID format** - Anonymous users now use valid UUID format
‚úÖ **FFmpeg parameters** - Improved conversion options (may need further work)

The audio pipeline should now work correctly for the happy path. Next steps are to test with real audio input and address any remaining transcription issues.

## Next Actions

1. **Restart backend** to pick up all changes:
   ```bash
   # Stop current server
   # Then restart
   make run-server
   ```

2. **Test in browser:**
   - Open frontend
   - Click microphone
   - Speak clearly
   - Wait for 1 second of silence
   - Verify transcription works

3. **Monitor logs:**
   - Frontend console: Check for audio chunking behavior
   - Backend logs: Check for FFmpeg and transcription errors

4. **If transcription still fails:**
   - Consider switching to WAV format temporarily
   - Add backend buffering for WebM chunks
   - Verify SenseVoice model is loaded correctly
