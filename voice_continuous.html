<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Continuous Voice Agent</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
        }
        
        .container {
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            max-width: 700px;
            width: 100%;
            padding: 40px;
        }
        
        h1 {
            color: #333;
            margin-bottom: 10px;
            font-size: 28px;
            text-align: center;
        }
        
        .subtitle {
            color: #666;
            text-align: center;
            margin-bottom: 30px;
            font-size: 14px;
        }
        
        .status {
            padding: 12px 20px;
            border-radius: 10px;
            margin-bottom: 20px;
            font-weight: 600;
            font-size: 14px;
            text-align: center;
            transition: all 0.3s ease;
        }
        
        .status.disconnected { background: #fee; color: #c33; }
        .status.connected { background: #efe; color: #3a3; }
        .status.active { background: #ffeaa7; color: #d63031; animation: pulse 1.5s infinite; }
        .status.agent-speaking { background: #e3f2fd; color: #1976d2; }
        
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.7; }
        }
        
        .button-wrapper {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin: 30px 0;
        }
        
        .voice-button {
            width: 140px;
            height: 140px;
            border-radius: 50%;
            border: none;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            font-size: 50px;
            cursor: pointer;
            box-shadow: 0 10px 30px rgba(102, 126, 234, 0.4);
            transition: all 0.3s ease;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        .voice-button:hover:not(:disabled) {
            transform: scale(1.05);
        }
        
        .voice-button.active {
            background: linear-gradient(135deg, #d63031 0%, #e17055 100%);
            animation: record-pulse 1s infinite;
        }
        
        @keyframes record-pulse {
            0%, 100% { box-shadow: 0 10px 30px rgba(214, 48, 49, 0.4); }
            50% { box-shadow: 0 10px 50px rgba(214, 48, 49, 0.8); }
        }
        
        .voice-button.listening {
            background: linear-gradient(135deg, #00b894 0%, #00cec9 100%);
        }
        
        .voice-button:disabled {
            background: #ccc;
            cursor: not-allowed;
        }
        
        .stop-button {
            width: 80px;
            height: 80px;
            border-radius: 15px;
            border: none;
            background: #e74c3c;
            color: white;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            box-shadow: 0 5px 15px rgba(231, 76, 60, 0.4);
            transition: all 0.3s ease;
            display: none;
        }
        
        .stop-button.visible {
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        .stop-button:hover {
            background: #c0392b;
            transform: scale(1.05);
        }
        
        .transcript-box {
            background: #f8f9fa;
            border-radius: 10px;
            padding: 20px;
            min-height: 250px;
            max-height: 400px;
            overflow-y: auto;
            margin: 20px 0;
        }
        
        .transcript-item {
            margin: 12px 0;
            padding: 12px;
            border-radius: 8px;
            font-size: 14px;
            line-height: 1.6;
            animation: fadeIn 0.3s ease;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        
        .transcript-item.you {
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
        }
        
        .transcript-item.agent {
            background: #f1f8e9;
            border-left: 4px solid #8bc34a;
        }
        
        .transcript-item.listening {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            font-style: italic;
            color: #666;
        }
        
        .transcript-item.interrupt {
            background: #ffebee;
            border-left: 4px solid #f44336;
            font-weight: 600;
        }
        
        .label {
            font-weight: 700;
            color: #333;
            margin-bottom: 6px;
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        
        .text {
            color: #555;
            font-size: 15px;
        }
        
        .empty-state {
            text-align: center;
            color: #999;
            padding: 40px;
            font-size: 14px;
        }
        
        .controls {
            display: flex;
            justify-content: center;
            gap: 10px;
            margin-top: 20px;
            flex-wrap: wrap;
        }
        
        .control-btn {
            background: #667eea;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 8px;
            cursor: pointer;
            font-size: 14px;
            transition: all 0.2s;
        }
        
        .control-btn:hover {
            background: #5568d3;
        }
        
        .note {
            background: #e8f5e9;
            border: 1px solid #81c784;
            border-radius: 8px;
            padding: 15px;
            margin-top: 20px;
            font-size: 13px;
            color: #666;
            line-height: 1.6;
        }
        
        .note strong {
            color: #2e7d32;
        }
        
        .note ul {
            margin: 10px 0 0 20px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üé§ Continuous Voice Agent</h1>
        <p class="subtitle">Click once to start, speak anytime, say "stop" to interrupt</p>
        
        <div id="status" class="status disconnected">Connecting...</div>
        
        <div class="button-wrapper">
            <button id="voiceBtn" class="voice-button" disabled>üé§</button>
            <button id="stopBtn" class="stop-button">‚èπ<br>STOP</button>
        </div>
        
        <div class="transcript-box" id="transcript">
            <div class="empty-state">
                Click the microphone button to start continuous conversation...<br>
                <small>The agent listens continuously and responds in real-time</small>
            </div>
        </div>
        
        <div class="controls">
            <button class="control-btn" onclick="clearTranscript()">Clear</button>
            <button class="control-btn" onclick="sendInterrupt()">Send Interrupt</button>
            <button class="control-btn" onclick="testCommand()">Test: "Latest news"</button>
        </div>
        
        <div class="note">
            <strong>üéØ Continuous Mode Instructions:</strong>
            <ul>
                <li>Click once to START continuous listening</li>
                <li>Speak naturally - the agent listens continuously</li>
                <li>Say "stop", "pause", or "quiet" to interrupt the agent</li>
                <li>Agent responds in real-time with streaming audio</li>
                <li>Click STOP button or say "exit" to end session</li>
            </ul>
        </div>
    </div>
    
    <audio id="audioPlayer" style="display: none;"></audio>

    <script>
        // Configuration
        const WS_URL = 'ws://localhost:8000/ws/voice';
        // Generate proper UUID v4
        const USER_ID = 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {
            const r = Math.random() * 16 | 0, v = c == 'x' ? r : (r & 0x3 | 0x8);
            return v.toString(16);
        });
        
        // State
        let ws = null;
        let sessionId = null;
        let recognition = null;
        let isSessionActive = false;
        let isAgentSpeaking = false;
        let audioQueue = [];
        let isPlaying = false;
        let lastTranscript = '';
        let silenceTimer = null;
        
        // Elements
        const voiceBtn = document.getElementById('voiceBtn');
        const stopBtn = document.getElementById('stopBtn');
        const statusDiv = document.getElementById('status');
        const transcriptDiv = document.getElementById('transcript');
        const audioPlayer = document.getElementById('audioPlayer');
        
        // Initialize
        window.onload = () => {
            initSpeechRecognition();
            connectWebSocket();
        };
        
        // Setup Continuous Speech Recognition
        function initSpeechRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            
            if (!SpeechRecognition) {
                alert('Speech Recognition not supported. Use Chrome or Edge.');
                return;
            }
            
            recognition = new SpeechRecognition();
            recognition.continuous = true;  // Keep listening continuously
            recognition.interimResults = true;
            recognition.lang = 'en-US';
            recognition.maxAlternatives = 1;
            
            recognition.onstart = () => {
                console.log('Continuous recognition started');
            };
            
            recognition.onresult = (event) => {
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const transcript = event.results[i][0].transcript.trim();
                    const isFinal = event.results[i].isFinal;
                    
                    if (transcript) {
                        if (!isFinal) {
                            // Show interim results
                            updateListeningIndicator(transcript);
                            clearSilenceTimer();
                        } else {
                            // Final result - send to backend
                            removeListeningIndicator();
                            if (transcript.toLowerCase() !== lastTranscript.toLowerCase()) {
                                lastTranscript = transcript;
                                handleFinalTranscript(transcript);
                            }
                        }
                    }
                }
            };
            
            recognition.onerror = (event) => {
                console.error('Recognition error:', event.error);
                if (event.error === 'no-speech') {
                    // Restart on no-speech
                    if (isSessionActive) {
                        setTimeout(() => recognition.start(), 100);
                    }
                }
            };
            
            recognition.onend = () => {
                console.log('Recognition ended');
                // Auto-restart if session is still active
                if (isSessionActive) {
                    setTimeout(() => {
                        try {
                            recognition.start();
                        } catch (e) {
                            console.error('Failed to restart recognition:', e);
                        }
                    }, 100);
                }
            };
        }
        
        // WebSocket Connection
        function connectWebSocket() {
            const url = `${WS_URL}?user_id=${USER_ID}`;
            console.log('Connecting to:', url);
            
            ws = new WebSocket(url);
            
            ws.onopen = () => {
                console.log('WebSocket connected');
                updateStatus('connected', 'Connected - Click to start conversation');
                voiceBtn.disabled = false;
            };
            
            ws.onmessage = (event) => {
                try {
                    const message = JSON.parse(event.data);
                    handleWebSocketMessage(message);
                } catch (e) {
                    console.error('Failed to parse message:', e);
                }
            };
            
            ws.onerror = (error) => {
                console.error('WebSocket error:', error);
                updateStatus('disconnected', 'Connection error');
            };
            
            ws.onclose = () => {
                console.log('WebSocket closed');
                stopSession();
                updateStatus('disconnected', 'Disconnected');
                voiceBtn.disabled = true;
                setTimeout(connectWebSocket, 3000);
            };
        }
        
        // Handle WebSocket Messages
        function handleWebSocketMessage(message) {
            const { event, data } = message;
            
            switch(event) {
                case 'connected':
                    sessionId = data.session_id;
                    console.log('Session ID:', sessionId);
                    break;
                    
                case 'listening_started':
                    updateStatus('active', 'üé§ Listening continuously... (say "stop" to interrupt)');
                    break;
                    
                case 'listening_stopped':
                    updateStatus('connected', 'Listening stopped');
                    break;
                    
                case 'transcription':
                    // Don't show, we already show it from local recognition
                    break;
                    
                case 'voice_response':
                    isAgentSpeaking = true;
                    updateStatus('agent-speaking', 'üîä Agent speaking... (say "stop" to interrupt)');
                    addTranscriptItem('agent', data.text);
                    break;
                    
                case 'tts_chunk':
                    playAudioChunk(data.audio_chunk);
                    break;
                    
                case 'streaming_complete':
                    isAgentSpeaking = false;
                    if (isSessionActive) {
                        updateStatus('active', 'üé§ Listening continuously...');
                    }
                    break;
                    
                case 'streaming_interrupted':
                    console.log('‚ö†Ô∏è Streaming interrupted by user');
                    isAgentSpeaking = false;
                    // Clear audio queue and stop playback immediately
                    // TODO: Frontend should also stop audio playback when receiving
                    // any new incoming message package from backend (e.g., new voice_response),
                    // not just on explicit interrupt events. This ensures smoother interruption.
                    audioQueue = [];
                    if (audioPlayer) {
                        audioPlayer.pause();
                        audioPlayer.src = '';
                    }
                    if (isSessionActive) {
                        updateStatus('active', 'üé§ Listening continuously...');
                    }
                    break;
                    
                case 'voice_interrupted':
                    isAgentSpeaking = false;
                    stopAllAudio();
                    addTranscriptItem('interrupt', '‚ö†Ô∏è Agent interrupted');
                    if (isSessionActive) {
                        updateStatus('active', 'üé§ Listening continuously...');
                    }
                    break;
                    
                case 'error':
                    console.error('Server error:', data);
                    break;
            }
        }
        
        // Handle Final Transcript
        function handleFinalTranscript(transcript) {
            console.log('Final transcript:', transcript);
            
            // Show in UI
            addTranscriptItem('you', transcript);
            
            // Check for interrupt commands
            const lowerTranscript = transcript.toLowerCase();
            if (lowerTranscript.includes('stop') || 
                lowerTranscript.includes('pause') || 
                lowerTranscript.includes('quiet') ||
                lowerTranscript.includes('silence')) {
                
                sendInterrupt();
                return;
            }
            
            // Check for exit command
            if (lowerTranscript.includes('exit') || 
                lowerTranscript.includes('quit') ||
                lowerTranscript.includes('goodbye')) {
                stopSession();
                return;
            }
            
            // Send command to backend
            sendCommand(transcript);
        }
        
        // Voice Button Click
        voiceBtn.onclick = () => {
            if (!isSessionActive) {
                startSession();
            } else {
                stopSession();
            }
        };
        
        // Stop Button Click
        stopBtn.onclick = () => {
            sendInterrupt();
        };
        
        // Start Continuous Session
        function startSession() {
            if (!recognition) return;
            
            try {
                isSessionActive = true;
                lastTranscript = '';
                
                // Update UI
                voiceBtn.classList.add('active');
                voiceBtn.textContent = 'üî¥';
                stopBtn.classList.add('visible');
                updateStatus('active', 'üé§ Listening continuously... (say "stop" to interrupt)');
                
                // Start recognition
                recognition.start();
                
                // Notify backend
                sendMessage({
                    event: 'start_listening',
                    data: {
                        session_id: sessionId
                    }
                });
                
                console.log('Continuous session started');
                
            } catch (error) {
                console.error('Error starting session:', error);
            }
        }
        
        // Stop Session
        function stopSession() {
            if (!isSessionActive) return;
            
            try {
                isSessionActive = false;
                
                // Stop recognition
                if (recognition) {
                    recognition.stop();
                }
                
                // Stop audio
                stopAllAudio();
                
                // Update UI
                voiceBtn.classList.remove('active', 'listening');
                voiceBtn.textContent = 'üé§';
                stopBtn.classList.remove('visible');
                updateStatus('connected', 'Session ended - Click to start again');
                
                // Notify backend
                sendMessage({
                    event: 'stop_listening',
                    data: {
                        session_id: sessionId
                    }
                });
                
                removeListeningIndicator();
                console.log('Continuous session stopped');
                
            } catch (error) {
                console.error('Error stopping session:', error);
            }
        }
        
        // Send Command
        /**
         * Send voice command to backend
         * 
         * Implements real-time interruption: if agent is currently speaking,
         * sends interrupt signal first, then waits briefly before sending new command.
         * This ensures the agent stops talking immediately when user speaks.
         */
        function sendCommand(text) {
            if (!text || !ws || ws.readyState !== WebSocket.OPEN) return;
            
            // If agent is speaking, interrupt first to stop TTS streaming
            if (isAgentSpeaking) {
                console.log('üõë Interrupting agent before sending new command');
                sendInterrupt();
                // Wait briefly for interrupt to process, then send command
                setTimeout(() => {
                    sendMessage({
                        event: 'voice_command',
                        data: {
                            session_id: sessionId,
                            command: text,
                            confidence: 0.95
                        }
                    });
                }, 100);
            } else {
                // Agent not speaking, send command directly
                sendMessage({
                    event: 'voice_command',
                    data: {
                        session_id: sessionId,
                        command: text,
                        confidence: 0.95
                    }
                });
            }
        }
        
        // Send Interrupt
        function sendInterrupt() {
            if (!ws || ws.readyState !== WebSocket.OPEN) return;
            
            stopAllAudio();
            
            sendMessage({
                event: 'interrupt',
                data: {
                    session_id: sessionId,
                    reason: 'user_interruption'
                }
            });
            
            addTranscriptItem('interrupt', '‚ö†Ô∏è You interrupted the agent');
        }
        
        // Send Message
        function sendMessage(message) {
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify(message));
            }
        }
        
        // Audio Playback
        function playAudioChunk(base64Audio) {
            audioQueue.push(base64Audio);
            if (!isPlaying) {
                playNextAudio();
            }
        }
        
        async function playNextAudio() {
            if (audioQueue.length === 0) {
                isPlaying = false;
                return;
            }
            
            isPlaying = true;
            const base64Audio = audioQueue.shift();
            
            try {
                const audioData = atob(base64Audio);
                const arrayBuffer = new Uint8Array(audioData.length);
                for (let i = 0; i < audioData.length; i++) {
                    arrayBuffer[i] = audioData.charCodeAt(i);
                }
                
                const blob = new Blob([arrayBuffer], { type: 'audio/mpeg' });
                const audioUrl = URL.createObjectURL(blob);
                
                audioPlayer.src = audioUrl;
                audioPlayer.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    playNextAudio();
                };
                
                await audioPlayer.play();
            } catch (err) {
                console.error('Failed to play audio:', err);
                playNextAudio();
            }
        }
        
        function stopAllAudio() {
            audioQueue = [];
            isPlaying = false;
            audioPlayer.pause();
            audioPlayer.currentTime = 0;
        }
        
        // UI Updates
        function updateStatus(state, text) {
            statusDiv.className = `status ${state}`;
            statusDiv.textContent = text;
        }
        
        function addTranscriptItem(type, text) {
            const empty = transcriptDiv.querySelector('.empty-state');
            if (empty) empty.remove();
            
            const item = document.createElement('div');
            item.className = `transcript-item ${type}`;
            
            const label = document.createElement('div');
            label.className = 'label';
            label.textContent = type === 'you' ? 'You' : 
                              type === 'agent' ? 'Agent' : 
                              type === 'interrupt' ? 'System' : 'Listening';
            
            const textDiv = document.createElement('div');
            textDiv.className = 'text';
            textDiv.textContent = text;
            
            item.appendChild(label);
            item.appendChild(textDiv);
            transcriptDiv.appendChild(item);
            transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
        }
        
        function updateListeningIndicator(text) {
            let indicator = document.getElementById('listeningIndicator');
            
            if (!indicator) {
                const empty = transcriptDiv.querySelector('.empty-state');
                if (empty) empty.remove();
                
                indicator = document.createElement('div');
                indicator.className = 'transcript-item listening';
                indicator.id = 'listeningIndicator';
                
                const label = document.createElement('div');
                label.className = 'label';
                label.textContent = 'Listening...';
                
                const textDiv = document.createElement('div');
                textDiv.className = 'text';
                
                indicator.appendChild(label);
                indicator.appendChild(textDiv);
                transcriptDiv.appendChild(indicator);
            }
            
            const textDiv = indicator.querySelector('.text');
            if (textDiv) {
                textDiv.textContent = text;
                transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
            }
            
            // Set silence timer
            clearSilenceTimer();
            silenceTimer = setTimeout(() => {
                // If no final result after 2 seconds, remove indicator
                removeListeningIndicator();
            }, 2000);
        }
        
        function removeListeningIndicator() {
            const indicator = document.getElementById('listeningIndicator');
            if (indicator) {
                indicator.remove();
            }
        }
        
        function clearSilenceTimer() {
            if (silenceTimer) {
                clearTimeout(silenceTimer);
                silenceTimer = null;
            }
        }
        
        function clearTranscript() {
            transcriptDiv.innerHTML = '<div class="empty-state">Transcript cleared...</div>';
        }
        
        function testCommand() {
            if (isSessionActive) {
                sendCommand('Tell me the latest news');
                addTranscriptItem('you', 'Tell me the latest news');
            } else {
                alert('Start the session first!');
            }
        }
    </script>
</body>
</html>

